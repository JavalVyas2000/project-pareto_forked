{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Machine Learning Surrogate\n",
    "\n",
    "To train a machine learning surrogate, we need to import several essential libraries. We will use `numpy` and `pandas` for data manipulation, and `tensorflow` for constructing and training the neural network. Additionally, we will utilize the `KerasSurrogate` object from the `idaes.core.surrogate` module in IDAES for surrogate modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import IDAES libraries\n",
    "from idaes.core.surrogate.sampling.data_utils import split_training_validation\n",
    "from idaes.core.surrogate.sampling.scaling import OffsetScaler\n",
    "from idaes.core.surrogate.keras_surrogate import (\n",
    "    KerasSurrogate,\n",
    "    save_keras_json_hd5,\n",
    "    load_keras_json_hd5,\n",
    ")\n",
    "from idaes.core.surrogate.plotting.sm_plotter import (\n",
    "    surrogate_scatter2D,\n",
    "    surrogate_parity,\n",
    "    surrogate_residual,\n",
    ")\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# fix environment variables to ensure consist neural network training\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "np.random.seed(46)\n",
    "rn.seed(1342)\n",
    "tf.random.set_seed(62)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the libraries, we will begin by reading the data generated from the Monte Carlo Simulation of the desalination model from an Excel file. We will then remove any points where the model was found to be infeasible using the `dropna` function from pandas. Given that the simulations are on the order of 1e3, we will use the entire dataset; however, users can choose the number of samples they want to use for training the surrogate model. Finally, we will define the input and output variables for the surrogate model and extract the input and output labels from the column names of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Auto-reformer training data\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "\n",
    "# Reading the data from excel and sampling from it.\n",
    "# mvc_simulation.xlsx is a dummy file name.\n",
    "# One should replace this with the actual file name\n",
    "csv_data = pd.read_excel(\"./mvc_simulations.xlsx\")\n",
    "csv_data.dropna(inplace=True)\n",
    "data = csv_data.sample(n=len(csv_data))\n",
    "\n",
    "# Defining the input and the output columns\n",
    "input_data = data[[\"Inlet\", \"Recovery\", \"Flow\"]]\n",
    "output_data = data[[\"CAPEX\", \"OPEX\"]]\n",
    "\n",
    "# Define labels\n",
    "input_labels = input_data.columns\n",
    "output_labels = output_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize the data to make sure the data is as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the dataframe to make sure things are in order\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to set up the neural network architecture, where we specify the activation function, optimizer, number of hidden layers, and number of nodes per layer. These are hyperparameters that should be configured by the user. We then define the loss function and the metrics for evaluating the model's performance. Since this is a regression model, we use mean squared error (MSE) as the loss function, and mean absolute error (MAE) and mean squared error (MSE) as the metrics.\n",
    "\n",
    "Next, we assign the input and output labels to variables `x` and `y`, respectively. We use a normalizing scaler to adjust the values by subtracting the mean and dividing by the standard deviation of each column. We then split the dataset into training and testing sets, using an 80/20 split.\n",
    "\n",
    "We create a Keras `Sequential` object and add the input layer, followed by the hidden layers, and finally the output layer. We compile the model by specifying the loss function, optimizer, and metrics. We also set up a checkpoint to save the state of the model, storing the best-known weights based on the minimum validation loss in the `.model_checkpoint.hdf5` file.\n",
    "\n",
    "Next, we fit the model using the training and validation data, specifying the number of epochs and the callback.\n",
    "\n",
    "We then determine the minimum and maximum values of each input variable in the dataset to set the bounds for the input variables. These bounds are passed to the `KerasSurrogate` object along with the neural network model, input and output labels, input and output scalers, and input bounds. Finally, we save the `KerasSurrogate` object to a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the hyper parameters to be used for training model\n",
    "activation, optimizer, n_hidden_layers, n_nodes_per_layer = \"relu\", \"Adam\", 3, 10\n",
    "loss, metrics = \"mse\", [\"mae\", \"mse\"]\n",
    "\n",
    "# Create data objects for training using scalar normalization\n",
    "n_inputs = len(input_labels)\n",
    "n_outputs = len(output_labels)\n",
    "x = input_data\n",
    "y = output_data\n",
    "\n",
    "# Scaling the data using normal scaler (x-xmin)/(xmax-xmin)\n",
    "# Converting scaled data to numpy from indexed series.\n",
    "input_scaler = None\n",
    "output_scaler = None\n",
    "input_scaler = OffsetScaler.create_normalizing_scaler(x)\n",
    "output_scaler = OffsetScaler.create_normalizing_scaler(y)\n",
    "x = input_scaler.scale(x)\n",
    "y = output_scaler.scale(y)\n",
    "x = x.to_numpy()\n",
    "y = y.to_numpy()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create Keras Sequential object and build neural network\n",
    "model = tf.keras.Sequential()\n",
    "model.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units=n_nodes_per_layer, input_dim=n_inputs, activation=activation\n",
    "    )\n",
    ")\n",
    "for i in range(1, n_hidden_layers):\n",
    "    model.add(tf.keras.layers.Dense(units=n_nodes_per_layer, activation=activation))\n",
    "model.add(tf.keras.layers.Dense(units=n_outputs))\n",
    "\n",
    "# Train surrogate (calls optimizer on neural network and solves for weights)\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "mcp_save = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \".model_checkpoint.hdf5\", save_best_only=True, monitor=\"val_loss\", mode=\"min\"\n",
    ")\n",
    "history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=2,\n",
    "    epochs=500,\n",
    "    callbacks=[mcp_save],\n",
    ")\n",
    "# save model to JSON and create callable surrogate object\n",
    "xmin, xmax = [0.00, 0.05, 0], [200, 0.95, 29.00]\n",
    "input_bounds = {input_labels[i]: (xmin[i], xmax[i]) for i in range(len(input_labels))}\n",
    "\n",
    "keras_surrogate = KerasSurrogate(\n",
    "    model,\n",
    "    input_labels=list(input_labels),\n",
    "    output_labels=list(output_labels),\n",
    "    input_bounds=input_bounds,\n",
    "    input_scaler=input_scaler,\n",
    "    output_scaler=output_scaler,\n",
    ")\n",
    "keras_surrogate.save_to_folder(\"keras_surrogate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we load the model from the folder and evaluate it using the entire dataset to assess the model's fit. This is done using the $R^2$ metric, where a value closer to 1 indicates a better fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model from the folder and evaluating it.\n",
    "surr = KerasSurrogate.load_from_folder(\"Keras_surrogate\")\n",
    "y_true = data.iloc[:, 3:5]  ### The true values from the dataframe.\n",
    "y_pred = surr.evaluate_surrogate(input_data)  ### The predicted values\n",
    "\n",
    "# Checking the R2 score to check the fit of the model.\n",
    "r = r2_score(y_true, y_pred)\n",
    "print(f\"The R2 score = {r:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we visualize the model using the `surrogate_scatter2D` plot to examine the fit. We also use the `surrogate_parity` plot to check the deviation of the model predictions from the ground truth and the `surrogate_residual` plot to analyze the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "surrogate_scatter2D(keras_surrogate, X_train, filename=\"keras_train_scatter2D.pdf\")\n",
    "surrogate_parity(keras_surrogate, X_train, filename=\"keras_train_parity.pdf\")\n",
    "surrogate_residual(keras_surrogate, X_train, filename=\"keras_train_residual.pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
